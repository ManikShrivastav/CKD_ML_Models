# -*- coding: utf-8 -*-
"""Data_pre-processing_&_Feature_Eng.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zz6LNzQnlaVSt7Tm2WEDoVxJDbcqO_9M

**Sanity check of data**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
import numpy as np

pd.set_option("display.max_column", None)
df = pd.read_excel("/content/Final_outcome.xlsx")
df.drop(df.columns[0], axis = 1, inplace = True)
df.head()

print(df.isnull().sum())

print(df.duplicated().sum())

print(df.info())

#Checking for any garbage values
for x in df.select_dtypes(include = "object").columns:
  print(df[x].value_counts())

"""**EDA**"""

Temporary_data = df.select_dtypes(exclude = "object")
Temporary_data.describe()

for x in df.select_dtypes(include = "number").columns:
  sn.boxplot(data = df, x = x)
  plt.show()

df.select_dtypes(include = "number").columns.to_list()

after_outlier = df
def wishker(col):
  q1,q3 = np.percentile(col,[25,75])
  iqr = q3 - q1
  lbound = q1 - (iqr * 1.5)
  ubound = q3 + (iqr * 1.5)
  return lbound,ubound
columns = after_outlier.select_dtypes(include="number").columns.drop(["class","su"], errors = "ignore").to_list()
for x in columns:
  lbound,ubound = wishker(after_outlier[x])
  after_outlier[x] = np.where(after_outlier[x] < lbound, lbound, after_outlier[x])
  after_outlier[x] = np.where(after_outlier[x] > ubound, ubound , after_outlier[x])
  sn.boxplot(data = after_outlier, x = x)
  plt.show()

from sklearn.preprocessing import LabelEncoder

objandcategory = after_outlier.select_dtypes(include = ['object', 'category']).columns

for col in objandcategory:
  Instance = LabelEncoder()
  after_outlier[col] = Instance.fit_transform(after_outlier[col])
after_outlier.head()

after_outlier.dtypes

"""**Feature Selection**"""

from sklearn.feature_selection import chi2

independent = after_outlier.select_dtypes(include = "number").drop(columns = ["class"], axis = 1)

dependent = after_outlier["class"]
scores = chi2(independent, dependent)
pd.DataFrame(scores)

#**Higher the Chi value ----> Higher the importance**
chivalues = pd.Series(scores[0], index = independent.columns)
chivalues.sort_values(ascending = True, inplace = True)
chivalues.plot.bar()

#**Higher the p-value -----> lower the importance**
pvalues = pd.Series(scores[1], index = independent.columns)
pvalues.sort_values(ascending = True, inplace = True)
pvalues.plot.bar()

plt.figure(figsize = (15,15))
sn.heatmap(after_outlier.corr(), cmap = 'plasma',  annot = True, cbar = False)
plt.title("Correlation matrix")
plt.show()

features = after_outlier.corr()
features = abs(features['class'])
features = features[features >= 0.29]
features = features.index[:-1]

features

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

X = after_outlier[features]
y = after_outlier['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svm_model = SVC(kernel='poly', degree=10, C=10000, random_state=42)
svm_model.fit(X_train_scaled, y_train)
y_pred = svm_model.predict(X_test_scaled)

accuracy_first_code = accuracy_score(y_test, y_pred)
print(f"First Code SVM Model Accuracy: {accuracy_first_code * 100:.2f}%")

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy * 100:.2f}%")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

scaler_standard = StandardScaler()
X_train_scaled_standard = scaler_standard.fit_transform(X_train)
X_test_scaled_standard = scaler_standard.transform(X_test)

param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto', 0.1, 1, 10]
}

grid_search = GridSearchCV(estimator=SVC(random_state=42), param_grid=param_grid, cv=5, verbose=1, n_jobs=-1)
grid_search.fit(X_train_scaled_standard, y_train)

best_svm_model = grid_search.best_estimator_
y_pred_tuned = best_svm_model.predict(X_test_scaled_standard)
accuracy_second_code_tuned = accuracy_score(y_test, y_pred_tuned)

print(f"Second Code Tuned Model Accuracy: {accuracy_second_code_tuned * 100:.2f}%")

print("Best hyperparameters:", grid_search.best_params_)

print(f"Accuracy: {accuracy_second_code_tuned * 100:.2f}%")
print("Confusion Matrix:")
conf_matrix = confusion_matrix(y_test, y_pred_tuned)
print(conf_matrix)

print("Classification Report:")
class_report = classification_report(y_test, y_pred_tuned)
print(class_report)

cm_first_code = confusion_matrix(y_test, y_pred)
disp_first_code = ConfusionMatrixDisplay(confusion_matrix=cm_first_code)
disp_first_code.plot(cmap='Blues', values_format='d')
plt.title('Confusion Matrix for First Code SVM Model')
plt.show()

cm_second_code_tuned = confusion_matrix(y_test, y_pred_tuned)
disp_second_code_tuned = ConfusionMatrixDisplay(confusion_matrix=cm_second_code_tuned)
disp_second_code_tuned.plot(cmap='Blues', values_format='d')
plt.title('Confusion Matrix for Tuned SVM Model')
plt.show()

plt.figure(figsize=(10, 6))
plt.bar(
    ['SVM Model (First Code)', 'Tuned Model (Second Code)'],
    [accuracy_first_code, accuracy_second_code_tuned],
    color=['red', 'orange']
)

plt.ylabel('Accuracy')
plt.title('SVM Model Accuracy Comparison')
plt.ylim(0, 1)
plt.tight_layout()
plt.xticks(rotation=45, ha='right')
plt.show()

import joblib

# Save the tuned model
joblib.dump(best_svm_model, 'svm_model_tuned.pkl')