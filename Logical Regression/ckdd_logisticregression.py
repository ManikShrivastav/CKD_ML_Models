# -*- coding: utf-8 -*-
"""CKDD_LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Aeg79T77j85Z8tN30cturH0gnVFy2ATT
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

data = pd.read_csv('/content/chronic_kidney_disease.csv')
data.head()

data.drop(data.columns[0], axis=1, inplace=True)

data.isnull().sum()

data.duplicated().sum()

# Check for any unwanted or invalid values in categorical columns in the 'data' dataframe
categorical_columns = data.select_dtypes(include="object").columns

for column in categorical_columns:
    print(f"Value counts for {column}:")
    print(data[column].value_counts())
    print("\n")

Temporary_data = data.select_dtypes(exclude="object")
Temporary_data.describe()

for x in data.select_dtypes(include="number").columns:
    sns.boxplot(data=data, x=x, color='green')
    plt.show()

def wishker(col):
    q1, q3 = np.percentile(col, [25, 75])
    iqr = q3 - q1
    lbound = q1 - (iqr * 1.5)
    ubound = q3 + (iqr * 1.5)
    return lbound, ubound

columns = data.select_dtypes(include="number").columns.drop(["class", "su"], errors="ignore").to_list()

for x in columns:
    lbound, ubound = wishker(data[x])
    data[x] = np.where(data[x] < lbound, lbound, data[x])
    data[x] = np.where(data[x] > ubound, ubound, data[x])
    sns.boxplot(data=data, x=x,color='green')
    plt.show()

from sklearn.preprocessing import LabelEncoder

objandcategory = data.select_dtypes(include=['object', 'category']).columns

for col in objandcategory:
    Instance = LabelEncoder()
    data[col] = Instance.fit_transform(data[col])

pd.set_option("display.max_column", None)
data.head()

plt.figure(figsize=(15, 15))
sns.heatmap(data.corr(), cmap='coolwarm', annot=True, cbar=False)
plt.title("Correlation Matrix")
plt.show()

correlations = data.corr().abs()
selected_features = correlations.loc[correlations['CKD'] >= 0.29, 'CKD']
selected_features = selected_features.index.difference(['CKD'])

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix
# Select only the relevant features from the dataset using selected_features
X = data[selected_features]
y = data['CKD']

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression without Scaling and Reduced Iterations
logistic_model = LogisticRegression(max_iter=10, random_state=42)  # Limited iterations
logistic_model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = logistic_model.predict(X_test)
accuracy_before_tuning = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Logistic Regression Model (Low Accuracy) Accuracy: {accuracy_before_tuning * 100:.2f}%")

# Plotting the confusion matrix using seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['True Negative', 'True Positive'])
plt.title('Confusion Matrix - Logistic Regression')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

X = data[selected_features]
y = data['CKD']

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for grid search
param_grid = {
    'solver': ['liblinear'],
    'max_iter': [50],
    'C': [0.01, 0.1, 1, 10, 100]  # Same C values for both grid search and plot
}

# Perform grid search
grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Best C value from the grid search
best_C = grid_search.best_params_['C']
best_accuracy = grid_search.best_score_

# Train the best model
best_logistic_model = grid_search.best_estimator_

# Predictions using the best model
y_pred_best = best_logistic_model.predict(X_test)

# Confusion Matrix for the best model
conf_matrix_best = confusion_matrix(y_test, y_pred_best)

# Plot confusion matrix using seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix_best, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['True Negative', 'True Positive'])
plt.title(f'Confusion Matrix - Best Logistic Regression (C={best_C})')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# Train and evaluate models over a range of C values
C_values = [0.01, 0.1, 1, 10, 100]
accuracies = []

for C in C_values:
    logistic_model = LogisticRegression(C=C, max_iter=50, solver='liblinear', random_state=42)
    logistic_model.fit(X_train, y_train)

    # Predictions and evaluation
    y_pred = logistic_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

# Plotting accuracy vs C values
plt.figure(figsize=(8, 6))
plt.plot(C_values, accuracies, marker='o', linestyle='-', color='b')
plt.title(f'Accuracy vs Regularization Strength (C) for Logistic Regression\nBest C: {best_C} with Accuracy: {best_accuracy * 100:.2f}%')
plt.xlabel('Regularization Strength (C)')
plt.ylabel('Accuracy')
plt.xscale('log')
plt.show()

# Output the results
print(f"Best C value from GridSearch: {best_C}")
print(f"Tuned Logistic Regression Model Accuracy: {best_accuracy * 100:.2f}%")

accuracies = [accuracy_before_tuning, best_accuracy]
models = ['Before Tuning', 'After Tuning']

plt.figure(figsize=(8, 6))
plt.bar(models, accuracies, color=['#4CAF50', '#FFC107'])
plt.title('Accuracy Comparison: Before vs After Hyperparameter Tuning')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.show()

import joblib

joblib.dump(best_logistic_model, 'best_logistic_model.pkl')

print("Model saved successfully!")